# -*- coding: utf-8 -*-
"""Copy of HW1 -- Blank.ipynb

Automatically generated by Colaboratory.

Original file is located at

# Behavior Cloning, DAGGER, CMA-ES, and GAIL


"""

from collections import OrderedDict 
import gym
import keras
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt
import numpy as np
import subprocess
import random
import tensorflow as tf
from tensorflow import set_random_seed
from numpy.random import seed

"""### Setup: Import Dependencies

### Make the TF Model
"""

def make_model():
  model = Sequential()      
  # WRITE CODE HERE
  # Add layers to the model:
  # a fully connected layer with 10 units
  # a tanh activation
  model.add(Dense(10, activation='tanh', input_shape=(4,)))

  # another fully connected layer with 2 units (the number of actions)
  # a softmax activation (so the output is a proper distribution)
  model.add(Dense(2, activation='softmax'))


  model.compile(loss='categorical_crossentropy',
                     optimizer=tf.train.AdamOptimizer(),
                     metrics=['accuracy'])
  
  # We expect the model to have four weight variables (a kernel and bias for
  # both layers)
  assert len(model.weights) == 4, 'Model should have 4 weights.'
  return model

"""### Test the model
To confirm that the model is correct
$$f(x) = \delta \left(\sum_{i=1}^4 x_i > 0 \right)$$

Achieved accuracy of at least 98%.
"""

model = make_model()
for t in range(20):
  X = np.random.normal(size=(1000, 4))  # some random data
  is_positive = np.sum(X, axis=1) > 0  # A simple binary function
  Y = np.zeros((1000, 2))
  Y[np.arange(1000), is_positive.astype(int)] = 1  # one-hot labels
  history = model.fit(X, Y, epochs=10, batch_size=256, verbose=0)
  loss = history.history['loss'][-1]
  acc = history.history['acc'][-1]
  print('(%d) loss= %.3f; accuracy = %.1f%%' % (t, loss, 100 * acc))

"""### Interacting with the Gym
Gathering an episode (a "rollout"). 
http://gym.openai.com/docs/#environments
"""

def action_to_one_hot(env, action):
    action_vec = np.zeros(env.action_space.n)
    action_vec[action] = 1
    return action_vec    
      
      
def generate_episode(env, policy):
  """Collects one rollout from the policy in an environment. The environment
  should implement the OpenAI Gym interface. A rollout ends when done=True. The
  number of states and actions should be the same.

  Args:
    env: an OpenAI Gym environment.
    policy: a keras model
  Returns:
    states: a list of states visited by the agent.
    actions: a list of actions taken by the agent.
    rewards: the reward received by the agent at each step.
  """
  done = False
  state = env.reset()

  states = []
  actions = []
  rewards = []
  while not done:

      action = policy.predict(np.array([state]))
      action = np.argmax(action)
      states.append(state)
      actions.append(action_to_one_hot(env, action))
      state, reward, done, info = env.step(action)
      rewards.append(reward)

      if done:
        # state = env.reset()
        break
  
  return np.array(states), np.array(actions), np.array(rewards)

"""### Test the data collection
"""

# Create the environment.
env = gym.make('CartPole-v0')
policy = make_model()
states, actions, rewards = generate_episode(env, policy)
assert len(states) == len(actions), 'Number of states and actions should be equal.'
assert len(actions) == len(rewards), 'Number of actions and rewards should be equal.'
assert len(actions[0]) == 2, 'Actions should use one-hot encoding.'
print('Test passed!')

"""### Download the expert policy
Run the cell below to upload the expert policy (`expert.h5`) to the Colab runtime.
"""

!wget https://raw.githubusercontent.com/cmudeeprl/703website/master/assets/homework/hw1/expert.h5

"""## Behavior Cloning and DAGGER (50 pt)

### Implementing Behavior Cloning and DAGGER
"""

class Imitation():

    def __init__(self, env, num_episodes):
        self.env = env
        self.expert = tf.keras.models.load_model('expert.h5')
        self.num_episodes = num_episodes
        
        
        self.model = make_model()
        
    def generate_behavior_cloning_data(self):
        self._train_states = []
        self._train_actions = []
        for _ in range(self.num_episodes):
            states, actions, rewards = generate_episode(self.env, self.expert)
            self._train_states.extend(states)
            self._train_actions.extend(actions)
        self._train_states = np.array(self._train_states)
        self._train_actions = np.array(self._train_actions)
        
    def generate_dagger_data(self):
        # Collect states and actions from the student policy
        # (self.model), and then relabel the actions using the expert policy.
        self._train_states = []
        self._train_actions = []
        policy = make_model()
        
        for _ in range(self.num_episodes):
          states, _, rewards = generate_episode(self.env, policy)
          actions_tmp = self.expert.predict(states)
          actions_tmp = np.argmax(actions_tmp, axis = 1)
          actions = np.zeros((len(actions_tmp), 2))
          actions[np.arange(len(actions_tmp)), actions_tmp.astype(int)] = 1
          self._train_states.extend(states)
          self._train_actions.extend(actions)
        
        self._train_states = np.array(self._train_states)
        self._train_actions = np.array(self._train_actions)
        
    def train(self, num_epochs=200):
        """Trains the model on training data generated by the expert policy.
        Args:
          env: The environment to run the expert policy on.
          num_epochs: number of epochs to train on the data generated by the expert.
        Return:
          loss: (float) final loss of the trained policy.
          acc: (float) final accuracy of the trained policy
        """
        
        history = self.model.fit(self._train_states, self._train_actions, epochs=num_epochs, verbose=0)
        loss = history.history['loss'][-1]
        acc = history.history['acc'][-1]
        #print('(%d) loss= %.3f; accuracy = %.1f%%' % (t, loss, 100 * acc))
        
        
        
        
        return loss, acc


    def evaluate(self, policy, n_episodes=50):
        rewards = []
        for i in range(n_episodes):
            _, _, r = generate_episode(self.env, policy)
            rewards.append(sum(r))
        r_mean = np.mean(rewards)
        return r_mean

"""### Experiment: Student vs Expert
Compare the performance of the expert policy
to the imitation policies obtained via behavior cloning and DAGGER.
"""

# Uncomment one of the two lines below to select whether to run behavior
# cloning or dagger
# mode = 'behavior cloning'
mode = 'dagger'

num_episodes = 100
num_iterations = 10  # Number of training iterations. Use a small number
                     # (e.g., 10) for debugging

# Create the environment.
env = gym.make('CartPole-v0')
im = Imitation(env, num_episodes)
expert_reward = im.evaluate(im.expert)
print('Expert reward: %.2f' % expert_reward)

loss_vec = []
acc_vec = []
imitation_reward_vec = []
for t in range(num_iterations):
  if mode == 'behavior cloning':
    im.generate_behavior_cloning_data()
  elif mode == 'dagger':
    im.generate_dagger_data()
  else:
    raise ValueError('Unknown mode: %s' % mode)
  loss, acc = im.train(num_epochs=1)
  imitation_reward = im.evaluate(im.model)
  loss_vec.append(loss)
  acc_vec.append(acc)
  imitation_reward_vec.append(imitation_reward)
  print('(%d) loss = %.3f; accuracy = %.2f; reward = %.1f' % (t, loss, acc, imitation_reward))



"""### Plot the results
"""

### Plot the results
plt.figure(figsize=(12, 3))
plt.subplot(131)
plt.title('Reward')
plt.plot(imitation_reward_vec, label='imitation')
plt.hlines(expert_reward, 0, len(imitation_reward_vec), label='expert')
plt.xlabel('iterations')
plt.ylabel('return')
plt.legend()
plt.ylim([0, None])

plt.subplot(132)
plt.title('Loss')
plt.plot(loss_vec)
plt.xlabel('iterations')
plt.ylabel('loss')

plt.subplot(133)
plt.title('Accuracy')
plt.plot(acc_vec)
plt.xlabel('iterations')
plt.ylabel('accuracy')
plt.tight_layout()
plt.savefig('student_vs_expert_%s.png' % mode, dpi=300)
plt.show()

"""### Experiment: How much expert data is needed?
Experimenting how the amount of expert data effects the performance. Varying the number of expert episodes collected at each iteration.
"""

random_seeds = 5
#numpy.random.seed
# Dictionary mapping number of expert trajectories to a list of rewards.
# Each is the result of running with a different random seed.
reward_data = OrderedDict({  
    1: [],
    10: [],
    50: [],
    100: []
})
accuracy_data = OrderedDict({  
    1: [],
    10: [],
    50: [],
    100: []
})
loss_data = OrderedDict({  
    1: [],
    10: [],
    50: [],
    100: []
})


for num_episodes in [1, 10, 50, 100]:
# for num_episodes in [ 100]:
  ne_loss_vec = []
  ne_acc_vec = []
  ne_reward_vec = []
  for tt in range(random_seeds):
    
    seed(tt)
    set_random_seed(tt)
    env.seed(tt)
    
#     mode = 'behavior cloning'
    mode = 'dagger'


    num_iterations = 100  # Number of training iterations. Use a small number
                         # (e.g., 10) for debugging

    # Create the environment.
    env = gym.make('CartPole-v0')
    im = Imitation(env, num_episodes)
    expert_reward = im.evaluate(im.expert)
    print('Expert reward: %.2f' % expert_reward)

    loss_vec = []
    acc_vec = []
    imitation_reward_vec = []
    for t in range(num_iterations):
      if mode == 'behavior cloning':
        im.generate_behavior_cloning_data()
      elif mode == 'dagger':
        im.generate_dagger_data()
      else:
        raise ValueError('Unknown mode: %s' % mode)
      loss, acc = im.train(num_epochs=1)
      imitation_reward = im.evaluate(im.model)
      loss_vec.append(loss)
      acc_vec.append(acc)
      imitation_reward_vec.append(imitation_reward)
      print('(%d) loss = %.3f; accuracy = %.2f; reward = %.1f' % (t, loss, acc, imitation_reward))
      
      if t > 5:
        pass_num = 0
        for i in range(t,t-5,-1):
          if (imitation_reward_vec[i] >= 199.5) and (acc_vec[i] >= 0.95):
            pass_num += 1
        if pass_num == 5:
          break
      
      
      
    print('num_episodes: %s; seed: %d' % (num_episodes, tt))

    ne_loss_vec.append(loss_vec) 
    ne_acc_vec.append(acc_vec) 
    ne_reward_vec.append(imitation_reward_vec)
  
#   print(num_episodes)
  reward_data[num_episodes].append(ne_reward_vec)
  accuracy_data[num_episodes].append(ne_acc_vec)
  loss_data[num_episodes].append(ne_loss_vec)

"""Plot the reward, loss, and accuracy for each, remembering to label each line."""

episode_list = [1,10,50,100]
seed_list = list(range(5))

def ave_func(ordered_dic, episode_list, seed_list):
    average_array = []
    for eipsode_num in episode_list:
        ave_vec = []
        iter_num = 0
        while True:
            agg_list = []
            for seed_num in seed_list:
                try:
                    agg_list.append(ordered_dic[eipsode_num][0][seed_num][iter_num])
                except:
                    pass
            iter_num += 1
            if len(agg_list) == 0:
                break
            else:
                ave_vec.append(np.mean(agg_list))
        average_array.append(ave_vec)

    return average_array


keys = [1, 10, 50, 100]
seed_color_dic = { 1 :"#bffffd", 10: "#f78383",50:"#c8ffbf", 100: "#8c8c8c"}
ave_color_dic = {1: "blue", 10: "red", 50: "green", 100: "black"}
for (index, (data, name)) in enumerate(zip([reward_data, accuracy_data, loss_data],
                                           ['reward', 'accuracy', 'loss'])):
    plt.figure(figsize=(12, 4))

    av_data = ave_func(data, episode_list, seed_list)
    iter = 0
    for episode_num in keys:

        for vec in data[episode_num][0]:
            plt.plot(vec, color=seed_color_dic[episode_num])
        plt.plot(av_data[iter], color=ave_color_dic[episode_num], label ="# of episodes: " + str(episode_num), marker = 'o', linewidth=2)
        iter += 1
    plt.legend()
    plt.xlabel("iterations")
    plt.ylabel(name)
    plt.title(name + " for " + mode)
    plt.show()

"""## CMA-ES
Black-box optimization algorithm. Maximizing function:
$$\max_\theta J(\theta) = E_{\pi_\theta} \left[ \sum_t r(s_t, a_t) \right]$$
"""

class CMAES:
    def __init__(self, env, L, n, p, sigma, noise, reward_fn=None):
        """
        Args:
          env: environment with which to evaluate different weights
          L: number of episodes used to evaluate a given set of weights
          n: number of members (weights) in each generation
          p: proportion of members used to update the mean and covariance
          sigma: initial std
          noise: additional noise to add to covariance
          reward_fn: if specified, this reward function is used instead of the 
            default environment reward. Used when the
            reward function will come from the discriminator. The reward
            function should be a function of the state and action.
        """

        self.env = env
        self.model = make_model()
        # Let d be the dimension of the 1d vector of weights.
        self.d = sum(int(np.product(w.shape)) for w in self.model.weights)
        self.mu = np.zeros(self.d)
        self.S = sigma**2 * np.eye(self.d)

        self.L = L
        self.n = n
        self.p = p
        self.noise = noise
        self.reward_fn = reward_fn


    def populate(self):
        """
        Populate a generation using the current estimates of mu and S
        """
        self.population = []
        
        self.population = np.random.multivariate_normal(self.mu, self.S, size = self.n)

#         print(np.shape(self.mu))
#         print('Mu:')
#         print(self.mu)
#         print('===========')
#         print(np.shape(self.S))
#         print('D:')
#         print(self.d)
#         print('-------------')

    def set_weight(self, member):
        ind = 0
        weights = []
        for w in self.model.weights:
          if len(w.shape) > 1:
            mat = member[ind:ind+int(w.shape[0]*w.shape[1])]
            mat = np.reshape(mat, w.shape)
            ind += int(w.shape[0]*w.shape[1])
          else:
            mat = member[ind:ind+int(w.shape[0])]
            ind += int(w.shape[0])
          weights.append(mat)

        self.model.set_weights(weights)
        

    def evaluate(self, member, num_episodes):
        """
        Evaluate a set of weights by interacting with the environment and
        return the average total reward over num_episodes.
        """
        self.set_weight(member)
        return self.evaluate_policy(self.model, num_episodes)
    
    def evaluate_policy(self, policy, num_episodes):
        episode_rewards = []
        for episode in range(num_episodes):
          # WRITE CODE HERE
          _, _, rewards = generate_episode(self.env, policy)
          episode_rewards.append(np.sum(rewards))

        return np.mean(episode_rewards)

    def train(self):
        """
        Perform an iteration of CMA-ES by evaluating all members of the
        current population and then updating mu and S with the top self.p
        proportion of members. Note that self.populate automatically deletes
        all the old members, so you no need to worry about deleting the
        "bad" members.

        """
        self.populate()
		
        rewards = []
        for member in self.population:
          r = self.evaluate(member,self.L)
          rewards.append(r)


        rewards = np.asarray(rewards)
        
        num_best_members = (int)(np.floor(self.n*self.p))
        best_indices = np.argpartition(rewards, -num_best_members)[-num_best_members:]
        best_indices = best_indices[np.argsort(-rewards[best_indices])]
        
        # print(best_indices)
        # print(rewards[best_indices])
        # best_indices = [4,1,2,5,8,9,10,12]

        self.mu = np.mean(self.population[best_indices,:],axis = 0)
        self.S  = np.cov(np.transpose(self.population[best_indices,:]))
        np.fill_diagonal(self.S, self.S.diagonal() + self.noise)

        # print("mu")
        # print(np.shape(self.mu))
        # print("S")
        # print(np.shape(self.S))

        best_member = self.population[best_indices[0],:]

        # print("bestMember")
        # print(np.shape(best_member))

        best_r = self.evaluate(best_member, 10)
        mu_r = self.evaluate(self.mu, 10)
        return mu_r, best_r



iterations = 200  # Use 10 for debugging
pop_size_vec = [50]
                              
data = {pop_size: [] for pop_size in pop_size_vec}

for pop_size in pop_size_vec:
  print('Population size:', pop_size)
  env = gym.make('CartPole-v0')
  optimizer = CMAES(env,
                    L=1,  # number of episodes for evaluation
                    n=pop_size,  # population size
                    p=0.25,  # proportion of population to keep
                    sigma=10,  # initial std dev
                    noise=0.25)  # noise

  for t in range(iterations):
      mu_r, best_r = optimizer.train()
      data[pop_size].append((mu_r, best_r))
      print('(%d) avg member rew = %.2f; best member rew = %.2f' % (t, mu_r, best_r))



plt.figure(figsize=(12, 4))
plt.subplot(121)  # Left plot will show best performance vs number of iterations
for pop_size, values in data.items():
  mu_r = np.array(values)[:, 1]  # performance of the best point
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(pop_size))
  plt.ylabel('best return', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)

plt.subplot(122)  # Right plot will show average performance vs number of iterations
for pop_size, values in data.items():
  mu_r = np.array(values)[:, 0]  # Uperformance of the avg point
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(pop_size))
  plt.ylabel('average return', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)

plt.legend()
plt.tight_layout()
plt.savefig('cmaes_best_average.png')
plt.show()



iterations = 200  
pop_size_vec = [20, 50, 100]
                              
data = {pop_size: [] for pop_size in pop_size_vec}

for pop_size in pop_size_vec:
  print('Population size:', pop_size)
  env = gym.make('CartPole-v0')
  optimizer = CMAES(env,
                    L=1,  # number of episodes for evaluation
                    n=pop_size,  # population size
                    p=0.25,  # proportion of population to keep
                    sigma=10,  # initial std dev
                    noise=0.25)  # noise

  for t in range(iterations):
      mu_r, best_r = optimizer.train()
      data[pop_size].append((mu_r, best_r))
      print('(%d) avg member rew = %.2f; best member rew = %.2f' % (t, mu_r, best_r))



plt.figure(figsize=(12, 4))
plt.subplot(121)  # Left plot will show performance vs number of iterations
for pop_size, values in data.items():
  mu_r = np.array(values)[:, 1]  # performance of the best point
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(pop_size), marker = 'o')
  plt.ylabel('best return', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)

plt.subplot(122)  # Right plot will show performance vs number of points evaluated
for pop_size, values in data.items():
  mu_r = np.array(values)[:, 0]  # performance of the average point
  x = pop_size * (np.arange(len(mu_r)) + 1)
  plt.plot(x, mu_r, label=str(pop_size), marker = 'o')
  plt.ylabel('best return', fontsize=16)
  plt.xlabel('num. points evaluated', fontsize=16)

plt.legend()
plt.tight_layout()
plt.savefig('cmaes_pop_size.png')
plt.show()

"""## Generative Adversarial Imitation Learning: GAIL
Only condition the discriminator on the state, not the action.
"""

class GAIL(object):
  
  
  
  def __init__(self, env):
    self.env = env
    self.expert = tf.keras.models.load_model('expert.h5')
    self.model = make_model()
    self.discriminator = make_model()
    self.cmaes = CMAES(env,
                  L=1,  # number of episodes for evaluation
                  n=20,  # population size
                  p=0.25,  # proportion of population to keep
                  sigma=10,  # initial std dev
                  noise=0.25,  # noise
                  reward_fn=self._reward_fn)
    self.expert_states = []
    self.student_states = []
    
  def _reward_fn(self, s, a):
    """Log probability that state is from expert."""
    del a
    p_expert = self.discriminator.predict(s[None])[0][0]
    return np.log(p_expert)
    # return np.log(p_expert+1)
  
  
  
  def collect_data(self, num_episodes):
    """Collect data from the expert and imitation policy. After the initial
    iteration, there is no need to collect new data from the expert, as the
    expert policy never changes.
    """
    collect_expert = len(self.expert_states) == 0
    self.student_states = []
    for _ in range(num_episodes):
      # Collect data from the expert policy
      # Collect data from the student policy
      if (collect_expert):
        states_expert, _, _ = generate_episode(self.env, self.expert)
        self.expert_states.extend(states_expert)
      
      states_student, _, _ = generate_episode(self.env, self.cmaes.model)
      self.student_states.extend(states_student)
  

  def train_discriminator(self, num_episodes):
    self.collect_data(num_episodes)
    X = np.concatenate((self.expert_states, self.student_states))
    
    Y_tmp = np.zeros(X.shape[0])
    Y_tmp[len(self.expert_states):] = 1
    Y = np.zeros((Y_tmp.shape[0], 2))
    Y[np.arange(Y_tmp.shape[0]), Y_tmp.astype(int)] = 1
    
    
    assert Y.shape[1] == 2  # 1-hot encoding for the labels
    assert np.all(np.sum(Y, axis=1) == 1)
    history = self.discriminator.fit(X, Y, epochs=10, batch_size=256, verbose=0)
    loss = history.history['loss'][-1]
    acc = history.history['acc'][-1]
    return loss, acc
  
  def train_policy(self):
    mu_r, best_r = self.cmaes.train()
    self.model = self.cmaes.model
    return mu_r, best_r

### Total variation distance to comparing two policies
def get_x_position_histogram(states):
  x_vec = [s[0] for s in states]  # The x position is the first coordinate
  bins = np.linspace(-2.4, 2.4, 11)
  hist, _ = np.histogram(x_vec, bins=bins, density=True)
  return hist

def TV_distance(expert_states, student_states):
  expert_hist = get_x_position_histogram(expert_states)
  student_hist = get_x_position_histogram(student_states)
  return 0.5 * np.sum(np.abs(expert_hist - student_hist))


def evaluate(gail):
  """Evaluating the policy learned by GAIL according to three metrics:
    1. Environment reward. We want this number to be large (~100)
    2. How well it fools the discriminator. In particular, we compute the
      discriminator's prediction that the policy is the expert. The policy
      tries to increase this number, while the discriminator tries to decrease
      it. We expect it to be around 40% - 60%
    3. Total variation distance between the student and the expert, along the
      X axis. We want this number to be small (~0)."""
  rewards_vec = []
  p_expert_vec = []
  for _ in range(10):
    states, actions, rewards = generate_episode(gail.env, gail.cmaes.model)
    rewards_vec.append(np.sum(rewards))
    log_p_expert = [gail._reward_fn(s, a) for (s, a) in zip(states, actions)]
    p_expert = np.exp(log_p_expert)
    p_expert_vec.append(np.mean(p_expert))
    ##

  tv_dist = TV_distance(gail.expert_states, states)
  return np.mean(p_expert_vec), np.mean(rewards_vec), tv_dist


env = gym.make('CartPole-v0')
gail = GAIL(env)
num_episodes = 20
discriminator_acc = []
for t in range(100):
  loss, accuracy = gail.train_discriminator(num_episodes)
  print('(%d) Loss: %.4f - Accuracy %.4f' % (t, loss, accuracy))
  discriminator_acc.append(accuracy)

discriminator_acc = np.array(discriminator_acc)
x = np.arange(len(discriminator_acc)) + 1
plt.plot(x, discriminator_acc)
plt.ylabel('Training Accuracy', fontsize=16)
plt.xlabel('num. iterations', fontsize=16)
plt.title('Training Accuracy of Discriminator')

plt.tight_layout()
plt.savefig('Discriminator_Training_Accuracy.png', dpi=300)
plt.show()


num_episodes = 20

policy_reward   = []
policy_tv       = []
policy_p_expert = []
for t in range(100):

  mu_r, best_r = gail.train_policy()

  p_expert, avg_r, tv_dist = evaluate(gail)

  policy_reward.append(avg_r)
  policy_tv.append(tv_dist)
  policy_p_expert.append(p_expert)
  print('(%d) Policy: p(expert) = %.2f%% ; reward = %.1f' % (t, 100.0 * p_expert, avg_r))
  print('(%d) Total variation distance = %.2f' % (t, tv_dist))

plt.figure()
policy_reward = np.array(policy_reward)
x = np.arange(len(policy_reward)) + 1
plt.plot(x, policy_reward)
plt.ylabel('Task Reward')
plt.xlabel('num. iterations')
plt.title('Task Reward throughout Training')
plt.show()

plt.figure()
policy_tv = np.array(policy_tv)
x = np.arange(len(policy_tv)) + 1
plt.plot(x, policy_tv)
plt.ylabel('TV Distance')
plt.xlabel('num. iterations')
plt.title('Total Variation Distance throughout Training')
plt.show()

plt.figure()
policy_p_expert = np.array(policy_p_expert)
x = np.arange(len(policy_p_expert)) + 1
plt.plot(x, policy_p_expert)
plt.ylabel('Expert Policy Probability')
plt.xlabel('num. iterations')
plt.title('Expert Policy Prediction Accuracy throughout Training') # Discriminator Decrease, Policy Increase
plt.show()


# plt.tight_layout()
plt.savefig('TV_TaskReward.png', dpi=300)
plt.show()


discriminator_update_period = 10
num_episodes = 20

env = gym.make('CartPole-v0')
gail = GAIL(env)

policy_reward   = []
policy_tv       = [] 
policy_p_expert = []
 
for t in range(100):
  if not (t % discriminator_update_period):
      loss, accuracy = gail.train_discriminator(num_episodes)
      print('(%d) Training Discriminator Loss: %.4f - Accuracy %.4f' % (t, loss, accuracy) )


  mu_r, best_r = gail.train_policy() 

  p_expert, avg_r, tv_dist = evaluate(gail)

  policy_reward.append(avg_r)
  policy_tv.append(tv_dist)
  policy_p_expert.append(p_expert)

  print('(%d) Policy: p(expert) = %.2f%% ; reward = %.1f' % (t, 100.0 * p_expert, avg_r))
  print('(%d) Total variation distance = %.2f' % (t, tv_dist))

plt.figure()
policy_reward = np.array(policy_reward)
x = np.arange(len(policy_reward)) + 1
plt.plot(x, policy_reward)
plt.ylabel('Task Reward')
plt.xlabel('num. iterations')
plt.title('GAIL - Task Reward throughout Training (UpdatePeriod=10)')
plt.show()

plt.figure()
policy_tv = np.array(policy_tv)
x = np.arange(len(policy_tv)) + 1
plt.plot(x, policy_tv)
plt.ylabel('TV Distance')
plt.xlabel('num. iterations')
plt.title('GAIL - Total Variation Distance throughout Training (UpdatePeriod=10)')
plt.show()

plt.figure()
policy_p_expert = np.array(policy_p_expert)
x = np.arange(len(policy_p_expert)) + 1
plt.plot(x, policy_p_expert)
plt.ylabel('Expert Policy Probability')
plt.xlabel('num. iterations')
plt.title('GAIL - Expert Policy Prediction Accuracy throughout Training (UpdatePeriod=10)') 
# Discriminator Decrease, Policy Increase

# plot.legend()
# plt.tight_layout()
plt.savefig('GAIL_TV_TaskReward.png', dpi=300)
plt.show()




discriminator_update_period = [2,5,10,20,50,100]
num_episodes = 20
env = gym.make('CartPole-v0')

discr_accuracy_data = []
discr_loss_data     = []
policy_mu_r_data   = []
policy_best_r_data = []
eval_p_expert = []
eval_avg_r    = []
eval_tv_dist  = []


for update_period in discriminator_update_period:

  gail = GAIL(env)

  discr_accuracy_data_curr = np.zeros((int)(np.floor(100/update_period)))
  discr_loss_data_curr     = np.zeros((int)(np.floor(100/update_period)))
  policy_mu_r_data_curr    = np.zeros(100) 
  policy_best_r_data_curr  = np.zeros(100)
  eval_p_expert_curr       = np.zeros(100)  
  eval_avg_r_curr          = np.zeros(100)  
  eval_tv_dist_curr        = np.zeros(100)  

  for t in range(100):
    if not (t % update_period):
      print('(%d) Training Discriminator for Update_Period = %d' % (t, update_period) )
      loss, accuracy = gail.train_discriminator(num_episodes)
      discr_accuracy_data_curr[divmod(t,update_period)[0]] = accuracy
      discr_loss_data_curr[divmod(t,update_period)[0]]     = loss

    mu_r, best_r = gail.train_policy() 
    policy_mu_r_data_curr[t]   = mu_r
    policy_best_r_data_curr[t] = best_r

    p_expert, avg_r, tv_dist = evaluate(gail)
    eval_p_expert_curr[t] = p_expert
    eval_avg_r_curr[t]    = avg_r
    eval_tv_dist_curr[t]  = tv_dist

  discr_accuracy_data.append(discr_accuracy_data_curr)
  discr_loss_data.append(discr_loss_data_curr)
  policy_mu_r_data.append(policy_mu_r_data_curr)
  policy_best_r_data.append(policy_best_r_data_curr)
  eval_p_expert.append(eval_p_expert_curr)
  eval_avg_r.append(eval_avg_r_curr)
  eval_tv_dist.append(eval_tv_dist_curr)

plt.figure()
for update_period_idx in range(len(eval_avg_r)):
  mu_r = np.array(eval_avg_r[update_period_idx])  # Average Reward
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(discriminator_update_period[update_period_idx]))
  plt.ylabel('Task Reward', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)
  plt.title('GAIL - Task Reward throughout Training for Different Update Periods')

plt.legend()
plt.show()

plt.figure()
for update_period_idx in range(len(eval_tv_dist)):
  mu_r = np.array(eval_tv_dist[update_period_idx])  # Average Reward
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(discriminator_update_period[update_period_idx]))
  plt.ylabel('TV Distance', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)
  plt.title('GAIL - Total Variation Distance throughout Training for Different Update Periods')

plt.legend()
plt.show()

plt.figure()
for update_period_idx in range(len(eval_p_expert)):
  mu_r = np.array(eval_p_expert[update_period_idx])  # Average Reward
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(discriminator_update_period[update_period_idx]))
  plt.ylabel('Expert Policy Probability', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)
  plt.title('GAIL - Expert Policy Prediction Accuracy throughout Training for Different Update Periods')
  # Discriminator Decrease, Policy Increase

plt.legend()
# plt.tight_layout()
plt.savefig('GAIL_TV_TaskReward_different_UpdateRate.png', dpi=300)
plt.show()

plt.figure()
for update_period_idx in range(len(eval_avg_r)-1):
  if update_period_idx == 0:
    continue
  mu_r = np.array(eval_avg_r[update_period_idx])  # Average Reward
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(discriminator_update_period[update_period_idx]))
  plt.ylabel('Task Reward', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)
  plt.title('GAIL - Task Reward throughout Training for Different Update Periods')

plt.legend()
plt.show()

plt.figure()
for update_period_idx in range(len(eval_tv_dist)-1):
  if update_period_idx == 0:
    continue
  mu_r = np.array(eval_tv_dist[update_period_idx])  # Average Reward
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(discriminator_update_period[update_period_idx]))
  plt.ylabel('TV Distance', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)
  plt.title('GAIL - Total Variation Distance throughout Training for Different Update Periods')

plt.legend()
plt.show()

plt.figure()
for update_period_idx in range(len(eval_p_expert)-1):
  if update_period_idx == 0:
    continue
  mu_r = np.array(eval_p_expert[update_period_idx])  # Average Reward
  x = np.arange(len(mu_r)) + 1
  plt.plot(x, mu_r, label=str(discriminator_update_period[update_period_idx]))
  plt.ylabel('Expert Policy Probability', fontsize=16)
  plt.xlabel('num. iterations', fontsize=16)
  plt.title('GAIL - Expert Policy Prediction Accuracy throughout Training for Different Update Periods')
  # Discriminator Decrease, Policy Increase

plt.legend()
# plt.tight_layout()
plt.savefig('GAIL_TV_TaskReward_different_UpdateRate.png', dpi=300)
plt.show()

"""#Done!"""